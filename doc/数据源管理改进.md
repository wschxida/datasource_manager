# 数据源管理改进

## listpage_url缩表（已完成）

### 问题

1. listpage_url 大量 无价值索引，只需要保留  Website_No, Client_ID两个索引 以及 Last_Update_Time，用复制rename的形式处理
2. 36G 大表，必须改为 Website_No分区表（按最后两个字母，100个分区），让查询高效



### 方案

1. 创建新表listpage_url_new，缩减字段，索引等，触发器和外键先不建，备份表listpage_url到118-->datasource;
2. 写Python，导数据从listpage_url 到 listpage_url_new，去除云爬虫的链接，之前存云爬虫链接的表cloud_listpage_url_backup20200911，
   所以sql如下：

select * from listpage_url l where 
not EXISTS (select ListPage_URL_ID from cloud_listpage_url_backup20200911 c where l.ListPage_URL_ID=c.ListPage_URL_ID)
and ListPage_URL_ID BETWEEN 0 and 123445
;

3. listpage_url重命名为listpage_url_backup，listpage_url_new重命名listpage_url

4. listpage_url复制数据到数据库117/118

5. 建触发器和外键，通知LHC修改同步字段（如果需要）

6. listpage_url保留索引：
   Record_GUID
   Last_Update_Time
   Created_Time
   Website_No
   Domain_Code
   ListPage_URL_MD5

   准备删除的索引：
   ListPage_URL(255)
   ListPage_Group_ID
   City_ID
   Country_ID
   District_ID
   Province_ID
   Client_ID
   ListPage_Save_Rule
   Order_No
   Host_Code
   Is_Enabled



### 结果

--处理结果--
listpage_url
处理前:
行 37602139
索引长度 20.57 GB (22,086,828,032)
数据长度 15.04 GB (16,149,643,264)

处理后:
行 21068597
索引长度 8.172134 GB
数据长度 10.169312 GB

--tips--
select * from View_Listpage_URL
where Is_Enabled=1 
and Website_No='S17611' 
;
这个采集的sql，由于Is_Enabled、Website_No都单独建了索引，误导了查询。
删了Is_Enabled索引之后查询速度提升了10倍。

界面php查询时，用了
WHERE l.Website_No = 'S18927' 
ORDER BY l.Last_Update_Time ASC
查询花了210s，应去掉Last_Update_Time 排序。建项目：
[ ]任务-28095-KWM_listpage_url界面查询速度优化, 0%, 计划 1小时/实际0小时, 开发部/开发部

### 相关目录

备份listpage_url代码：

\\192.168.1.7\乐思当前项目中心\2020项目\3月项目\P25352_【KWM十大问题】\数据源\output_listpage_116_to_118.py

删除listpage_url代码：

\\192.168.1.7\乐思当前项目中心\2020项目\3月项目\P25352_【KWM十大问题】\数据源\delete_data_listpage.py



## listpage_url_pusher

### 问题

整体对各类型的 数据源统一管理： 可以把指定条件的数据源推入指定Redis队列

乱序机制 ，采列表 不能这么快，比如一个网站 100个列表页，不能10个线程并发采集，而是乱序，不同域名的并发，时间上错开

### 方案

1. 查询listpage_url的SQL改一下就好，乱序排序



### 结果



### 相关目录





## listpage_url_checker

### 问题

下面的部分如果做了，告知Bear，变通处理

3. 这个表的 无效网址必须 在数据源管理中 （建一个库？） 单独标记出来，文本采集，DOM采集无效的，整条记录转入  listpage_url_invalid表，从listpage_url 中删除
   这个检查，每周检查，可以指定网站编号集合检查
    TargetURLChecker
    InvalidURLMover

这样避免 资源浪费

4. 单独建立一个表，记录哪些网站移除了URL，移除了多少个，还剩多少个（是否要自动扩展栏目），日期
   把这个数据提供给kwmcloud，进行可视化监测，设计表SQL给Bear 

5. 重要网站的数据，开发部将放入这个表 stat_important_website 

### 方案

```
取页面前10个文本长度大于10的a节点计算MD5，对比7天前的MD5，假如一致就说明页面大概率是停更状态
```

### 结果

### 相关目录

  



## reject_domain_cleaner

### 问题

云采集的垃圾信息太多



### 方案

1. 云采集 的 垃圾域名事先删除，



### 结果

### 相关目录



## new_listpage_finder

### 问题

http://www.ifeng.com/

很多数据没有1. 栏目肯定不全，人为肯定不全 2. 调度不及时，3 采集太慢 

对于这类中型 网站如何 高速自动扫描新链接 ？

1. 自动发现新链接的同时，就要发现短链接栏目 【中文多少词，英文多少词】，
2. 短链接 是否 有效列表页面判断 ：文本长度>10的链接至少要有? 个 
3. 栏目 更新率 判断 ： 最近1天是否有新链接？最近3天是否有新链接；3天都没有的1天一次，7天都没有的  2天一次
4. 栏目在，但是无新数据

### 方案

1.每次采集，从入口website开始，采3层，临时存放在new_listpage_url_cache

2.采完之后，插入 new_listpage_url，做唯一性判断

3.每天一次，从118查出当天入库的new_listpage_url, 插入到中心库listpage_url。这样每天只插入新增的，而不需要一个网站每次都全部重置。



发现新栏目程序已经改进，算法思路大致如下：
1.从首页采集两层
2.获取页面所有a节点，
如果a节点数小于50，取前50%的节点；
如果a节点数大于50小于200，取前30%的节点；
如果a节点数大于200，取前20%的节点；
3.通过标题长度、标题关键字、URL长度、URL层级、URL关键字等方面对栏目进行评分，
满分100，暂定大于20分入库

### 结果

### 相关目录

